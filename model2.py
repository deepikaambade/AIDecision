# -*- coding: utf-8 -*-
"""model2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JgOU2d7GAucvcZKNvHOnnpU01xOuPrDp

<a id='top'></a>
<div class="list-group" id="list-tab" role="tablist">
    
<h1 style="padding: 8px;color:white; display:fill;background-color:#555555; border-radius:5px; font-size:150%"><b> Table of contents </b></h1>

 - [Introduction](#1)

 - [Explore](#2)

 - [Pre-processing and feature selection](#3)
    
 - [Modelling and Evaluation](#4)
    - [Logistical Classification](#4_1)
    - [Decision Tree](#4_3)
    - [Random Forest](#4_5)
    - [Gradient Boosting Classifier](#4_6)
    - [Neural Network MLP](#4_7)
    
 - [Evaluate](#5)
 - [Conclusion](#6)

<a id='1'></a>
# <p style="padding: 8px;color:white; display:fill;background-color:#555555; border-radius:5px; font-size:100%"> <b>Introduction</b>

This synthetic dataset is modeled after an existing milling machine and consists of 10 000 data points from a stored as rows with 14 features in columns

1. UID: unique identifier ranging from 1 to 10000
2. product ID: consisting of a letter L, M, or H for low (50% of all products), medium (30%) and high (20%) as product quality variants and a variant-specific serial number
3. type: just the product type L, M or H from column 2
4. air temperature [K]: generated using a random walk process later normalized to a standard deviation of 2 K around 300 K
5. process temperature [K]: generated using a random walk process normalized to a standard deviation of 1 K, added to the air temperature plus 10 K.
6. rotational speed [rpm]: calculated from a power of 2860 W, overlaid with a normally distributed noise
7. torque [Nm]: torque values are normally distributed around 40 Nm with a SD = 10 Nm and no negative values.
8. tool wear [min]: The quality variants H/M/L add 5/3/2 minutes of tool wear to the used tool in the process.
9. a 'machine failure' label that indicates, whether the machine has failed in this particular datapoint for any of the following failure modes are true.

The machine failure consists of five independent failure modes

1. tool wear failure (TWF): the tool will be replaced of fail at a randomly selected tool wear time between 200 - 240 mins (120 times in our dataset). At this point in time, the tool is replaced 69 times, and fails 51 times (randomly assigned).
2. heat dissipation failure (HDF): heat dissipation causes a process failure, if the difference between air- and process temperature is below 8.6 K and the tools rotational speed is below 1380 rpm. This is the case for 115 data points.
3. power failure (PWF): the product of torque and rotational speed (in rad/s) equals the power required for the process. If this power is below 3500 W or above 9000 W, the process fails, which is the case 95 times in our dataset.
4. overstrain failure (OSF): if the product of tool wear and torque exceeds 11,000 minNm for the L product variant (12,000 M, 13,000 H), the process fails due to overstrain. This is true for 98 datapoints.
5. random failures (RNF): each process has a chance of 0,1 % to fail regardless of its process parameters. This is the case for only 5 datapoints, less than could be expected for 10,000 datapoints in our dataset.

If at least one of the above failure modes is true, the process fails and the 'machine failure' label is set to 1. It is therefore not transparent to the machine learning method, which of the failure modes has caused the process to fail.

<a id='2'></a>
# <p style="padding: 8px;color:white; display:fill;background-color:#555555; border-radius:5px; font-size:100%"> <b>Explore</b>

1. Imports essential libraries like `numpy`, `pandas`, `matplotlib`, `seaborn`, and `plotly` for data analysis and visualization.  
2. Configures Pandas to display all columns, up to 500 rows, and prevents DataFrame wrapping.  
3. Modifies Jupyter Notebook output to have a scrollable height of `35em` for better readability.  
4. Reloads `matplotlib` settings and ensures plots appear inline with high resolution (`retina`).  
5. Suppresses warnings to keep the notebook output clean.  
6. Sets Plotly's default renderer to `iframe`, ensuring compatibility with Jupyter.  
7. Creates a custom Plotly template (`ck_template`) with the Viridis color scheme, fixed size (800×600), and Calibri font.  
8. Applies `ck_template+gridon` as the default Plotly theme, adding grid lines for better readability.  
9. Provides an alternative template (`seaborn+gridon`), which is commented out.  
10. Optimizes the notebook environment for efficient data analysis and visualization. 🚀
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

from IPython.core.display import display, HTML
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.io as pio


import seaborn as sns
from importlib import reload
import matplotlib.pyplot as plt
import matplotlib
import warnings

# Configure Jupyter Notebook
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 500)
pd.set_option('display.expand_frame_repr', False)
# pd.set_option('max_colwidth', -1)
display(HTML("<style>div.output_scroll { height: 35em; }</style>"))

reload(plt)
# %matplotlib inline
# %config InlineBackend.figure_format ='retina'

warnings.filterwarnings('ignore')

# configure plotly graph objects
pio.renderers.default = 'iframe'
# pio.renderers.default = 'vscode'

pio.templates["ck_template"] = go.layout.Template(
    layout_colorway = px.colors.sequential.Viridis,
#     layout_hovermode = 'closest',
#     layout_hoverdistance = -1,
    layout_autosize=False,
    layout_width=800,
    layout_height=600,
    layout_font = dict(family="Calibri Light"),
    layout_title_font = dict(family="Calibri"),
    layout_hoverlabel_font = dict(family="Calibri Light"),
#     plot_bgcolor="white",
)

# pio.templates.default = 'seaborn+ck_template+gridon'
pio.templates.default = 'ck_template+gridon'
# pio.templates.default = 'seaborn+gridon'
# pio.templates

df = pd.read_csv('ai4i2020.csv')
# df = pd.read_csv('ai4i2020.csv')

"""First up is just to eyeball the data. It seems that there are two indices: the index and ProductID. We can drop those. There is a Type which is categorical and the remainder are numeric. The last five feastures are all failure modes, so they will not be evaluated in this notebook."""

df.head()

"""There are no apparent missing values, but we'll check these out carefully to make sure"""

df.info()

df.describe(include='all').T

"""making sure that there are no missing values hidden as a question mark"""

df.replace("?",np.nan,inplace=True)

"""turn all columns into float to make processing later easier"""

for column in df.columns:
    try:
        df[column]=df[column].astype(float)
    except:
        pass

"""just check the descriptions for the numeric features. None missing and on apparent outliers"""

# show the numeric characters
df_numeric = df.select_dtypes(include=[np.number])
df_numeric.describe(include='all').T

"""Another verification whether there are any missing features. I see none."""

plt.figure(figsize=(15,15))
plot_kws={"s": 1}
sns.heatmap(df.isna().transpose(),
            cmap='cividis',
            linewidths=0.0,
           ).set_facecolor('white')

"""There are strongly correlated features namely process and air temperature. Torque and rotational speed are also strongly correlated. We can drop one of the temperatures, but the torque to rotational speed difference might be a indication of a failure, so we'll keep both.  """

plt.figure(figsize=(10,10))
threshold = 0.80
sns.set_style("whitegrid", {"axes.facecolor": ".0"})
df_cluster2 = df.select_dtypes(include=np.number).corr()
mask = df_cluster2.where((abs(df_cluster2) >= threshold)).isna()
plot_kws={"s": 1}
sns.heatmap(df_cluster2,
            cmap='RdYlBu',
            annot=True,
            mask=mask,
            linewidths=0.2,
            linecolor='lightgrey').set_facecolor('white')

!pip install ydata_profiling

from ydata_profiling import ProfileReport

"""The profiling report follows to look for outliers, missing values, and distributions. We can see that the data is imbalanced."""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# profile = ProfileReport(df,
#                         title="Predictive Maintenance",
#                         dataset={"description": "This profiling report was generated for Deepika Ambade"},
#                         explorative=True,
#                        )
# profile

"""Drop the indices as these have no predictive power"""

df.drop(['UDI','Product ID'],axis=1,inplace=True)

"""Drop the failure modes, as we're only interested whether something is a failure. I guess that you'll build a model for each failure mode if it comes down to that."""

df.drop(['TWF','HDF','PWF','OSF','RNF'],axis=1,inplace=True)

"""Drop the type, as this dominates too strongly on type = L."""

df.drop(['Type'],axis=1,inplace=True)

"""The remaining features"""

list(df)

"""turn categorical information into numeric"""

df = pd.get_dummies(df,drop_first=True)

features = list(df.columns)

for feature in features:
    print(feature + " - " + str(len(df[df[feature].isna()])))

"""Just another confirmation of how badly imbalanced the data is. We'll need to oversample in this case to get a better prediction."""

df_group = df.groupby(['Machine failure'])
df_group.count()

df[df['Machine failure'].isna()]

"""<a id='3'></a>
# <p style="padding: 8px;color:white; display:fill;background-color:#555555; border-radius:5px; font-size:100%"> <b>Pre-processing and Feature Selection</b>

replace the missing numeric values with the mean
"""

df_numeric.fillna(df_numeric.mean(),inplace=True)

for feature in features:
    try:
        df[feature].fillna(df[feature].mean(),inplace=True)
    except:
        try:
            df[feature].fillna(df[feature].mode(),inplace=True)
        except:
            pass

df.describe(include='all').T

"""# **UNIVARIATE ANALYSIS**
Perform a statistical univariate test to determine the best features. Product type L dominates this strongly.
"""

# Feature Selection
from sklearn.feature_selection import SelectKBest, chi2

best_features = SelectKBest(score_func=chi2,k='all')

X = df.iloc[:,:-1]
y = df.iloc[:,-1]
fit = best_features.fit(X,y)

df_scores=pd.DataFrame(fit.scores_)
df_col=pd.DataFrame(X.columns)

feature_score=pd.concat([df_col,df_scores],axis=1)
feature_score.columns=['feature','score']
feature_score.sort_values(by=['score'],ascending=True,inplace=True)

fig = go.Figure(go.Bar(
            x=feature_score['score'][0:21],
            y=feature_score['feature'][0:21],
            orientation='h'))

fig.update_layout(title="Top 20 Features",
                  height=1200,
                  showlegend=False,
                 )

fig.show()

Selected_Features = []
import statsmodels.api as sm

def backward_regression(X, y, initial_list=[], threshold_out=0.5, verbose=True):
    """To select feature with Backward Stepwise Regression

    Args:
        X -- features values
        y -- target variable
        initial_list -- features header
        threshold_out -- pvalue threshold of features to drop
        verbose -- true to produce lots of logging output

    Returns:
        list of selected features for modeling
    """
    included = list(X.columns)
    while True:
        changed = False
        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()
        # use all coefs except intercept
        pvalues = model.pvalues.iloc[1:]
        worst_pval = pvalues.max()  # null if pvalues is empty
        if worst_pval > threshold_out:
            changed = True
            worst_feature = pvalues.idxmax()
            included.remove(worst_feature)
            if verbose:
                print(f"worst_feature : {worst_feature}, {worst_pval} ")
        if not changed:
            break
    Selected_Features.append(included)
    print(f"\nSelected Features:\n{Selected_Features[0]}")


# Application of the backward regression function on our training data
backward_regression(X, y)

X = df.iloc[:,:-1]
y = df.iloc[:,-1]

X.head()
feature_names = list(X.columns)
np.shape(X)

np.shape(X)

len(feature_names)

"""<a id='4'></a>
# <p style="padding: 8px;color:white; display:fill;background-color:#555555; border-radius:5px; font-size:100%"> <b>Modelling and Evaluation</b>

The data is too imbalanced, therefore we usually oversample. The randomoversampler performs better than SMOTE, but not oversampling performs the best. This is curious, but there are studies (check my discussion here: https://www.kaggle.com/competitions/autismdiagnosis/discussion/322588) that reckon that it is better not to oversample.
"""

# import library
from imblearn.over_sampling import SMOTE, SVMSMOTE,RandomOverSampler
oversamp = RandomOverSampler(random_state=0)
# oversamp = SMOTE(n_jobs=-1)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size = 0.2,
                                                    random_state = 0,
                                                    stratify=y)
# X_train,y_train = oversamp.fit_resample(X_train, y_train)

"""There are no distinct outliers, therefore a simple minmax scaler suffices."""

from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score,roc_auc_score,matthews_corrcoef
from sklearn.metrics import ConfusionMatrixDisplay # Import ConfusionMatrixDisplay instead of plot_confusion_matrix

import time
model_performance = pd.DataFrame(columns=['Accuracy','Recall','Precision','F1-Score','MCC score','time to train','time to predict','total time'])

"""<a id='4_1'></a>
## <p style="padding: 8px;color:white; display:fill;background-color:#aaaaaa; border-radius:5px; font-size:100%"> <b>Logistical Classification</b>
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.linear_model import LogisticRegression
# start = time.time()
# model = LogisticRegression().fit(X_train,y_train)
# end_train = time.time()
# y_predictions = model.predict(X_test) # These are the predictions from the test data.
# end_predict = time.time()

accuracy = accuracy_score(y_test, y_predictions)
recall = recall_score(y_test, y_predictions, average='weighted')
precision = precision_score(y_test, y_predictions, average='weighted')
f1s = f1_score(y_test, y_predictions, average='weighted')
MCC = matthews_corrcoef(y_test, y_predictions)
# ROC_AUC = roc_auc_score(y_test, y_predictions, average='weighted')
ROC_AUC = roc_auc_score(y_test, model.predict_proba(X_test)[:,1], average='weighted')

print("Accuracy: "+ "{:.2%}".format(accuracy))
print("Recall: "+ "{:.2%}".format(recall))
print("Precision: "+ "{:.2%}".format(precision))
print("F1-Score: "+ "{:.2%}".format(f1s))
print("MCC: "+ "{:.2%}".format(MCC))
print("ROC AUC score: "+ "{:.2%}".format(ROC_AUC))
print("time to train: "+ "{:.2f}".format(end_train-start)+" s")
print("time to predict: "+"{:.2f}".format(end_predict-end_train)+" s")
print("total: "+"{:.2f}".format(end_predict-start)+" s")
model_performance.loc['Logistic'] = [accuracy, recall, precision, f1s,MCC,end_train-start,end_predict-end_train,end_predict-start]

plt.rcParams['figure.figsize']=5,5
sns.set_style("white")

# Instead of plot_confusion_matrix, use ConfusionMatrixDisplay.from_estimator: because it was giving me error
from sklearn.metrics import ConfusionMatrixDisplay  # Ensure this is imported
ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=plt.cm.Blues)

plt.show()

"""<a id='4_3'></a>
## <p style="padding: 8px;color:white; display:fill;background-color:#aaaaaa; border-radius:5px; font-size:100%"> <b>Decision Tree</b>
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.tree import DecisionTreeClassifier
# start = time.time()
# model = DecisionTreeClassifier().fit(X_train,y_train)
# end_train = time.time()
# y_predictions = model.predict(X_test) # These are the predictions from the test data.
# end_predict = time.time()

accuracy = accuracy_score(y_test, y_predictions)
recall = recall_score(y_test, y_predictions, average='weighted')
precision = precision_score(y_test, y_predictions, average='weighted')
f1s = f1_score(y_test, y_predictions, average='weighted')
MCC = matthews_corrcoef(y_test, y_predictions)
# ROC_AUC = roc_auc_score(y_test, y_predictions, average='weighted')
ROC_AUC = roc_auc_score(y_test, model.predict_proba(X_test)[:,1], average='weighted')

print("Accuracy: "+ "{:.2%}".format(accuracy))
print("Recall: "+ "{:.2%}".format(recall))
print("Precision: "+ "{:.2%}".format(precision))
print("F1-Score: "+ "{:.2%}".format(f1s))
print("MCC: "+ "{:.2%}".format(MCC))
print("ROC AUC score: "+ "{:.2%}".format(ROC_AUC))
print("time to train: "+ "{:.2f}".format(end_train-start)+" s")
print("time to predict: "+"{:.2f}".format(end_predict-end_train)+" s")
print("total: "+"{:.2f}".format(end_predict-start)+" s")
model_performance.loc['Decision Tree'] = [accuracy, recall, precision, f1s,MCC,end_train-start,end_predict-end_train,end_predict-start]

!pip install scikit-learn --upgrade

plt.rcParams['figure.figsize']=5,5
sns.set_style("white")

# Instead of plot_confusion_matrix, use ConfusionMatrixDisplay.from_estimator: because it was giving me error
from sklearn.metrics import ConfusionMatrixDisplay  # Ensure this is imported
ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=plt.cm.Blues)

plt.show()

plt.rcParams['figure.figsize']=10,10
sns.set_style("white")
feat_importances = pd.Series(model.feature_importances_, index=feature_names)
feat_importances = feat_importances.groupby(level=0).mean()
feat_importances.nlargest(20).plot(kind='barh').invert_yaxis()
sns.despine()
plt.show()

"""<a id='4_5'></a>
## <p style="padding: 8px;color:white; display:fill;background-color:#aaaaaa; border-radius:5px; font-size:100%"> <b>Random Forest</b>
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.ensemble import RandomForestClassifier
# start = time.time()
# model = RandomForestClassifier(n_estimators = 100,n_jobs=-1,random_state=0,bootstrap=True,).fit(X_train,y_train)
# end_train = time.time()
# y_predictions = model.predict(X_test) # These are the predictions from the test data.
# end_predict = time.time()

accuracy = accuracy_score(y_test, y_predictions)
recall = recall_score(y_test, y_predictions, average='weighted')
precision = precision_score(y_test, y_predictions, average='weighted')
f1s = f1_score(y_test, y_predictions, average='weighted')
MCC = matthews_corrcoef(y_test, y_predictions)
# ROC_AUC = roc_auc_score(y_test, y_predictions, average='weighted')
ROC_AUC = roc_auc_score(y_test, model.predict_proba(X_test)[:,1], average='weighted')

print("Accuracy: "+ "{:.2%}".format(accuracy))
print("Recall: "+ "{:.2%}".format(recall))
print("Precision: "+ "{:.2%}".format(precision))
print("F1-Score: "+ "{:.2%}".format(f1s))
print("MCC: "+ "{:.2%}".format(MCC))
print("ROC AUC score: "+ "{:.2%}".format(ROC_AUC))
print("time to train: "+ "{:.2f}".format(end_train-start)+" s")
print("time to predict: "+"{:.2f}".format(end_predict-end_train)+" s")
print("total: "+"{:.2f}".format(end_predict-start)+" s")
model_performance.loc['Random Forest'] = [accuracy, recall, precision, f1s,MCC,end_train-start,end_predict-end_train,end_predict-start]

plt.rcParams['figure.figsize']=5,5
sns.set_style("white")

# Instead of plot_confusion_matrix, use ConfusionMatrixDisplay.from_estimator: because it was giving me error
from sklearn.metrics import ConfusionMatrixDisplay  # Ensure this is imported
ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=plt.cm.Blues)

plt.show()

"""<a id='4_6'></a>
## <p style="padding: 8px;color:white; display:fill;background-color:#aaaaaa; border-radius:5px; font-size:100%"> <b>Gradient Boosting Classifier</b>
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.ensemble import GradientBoostingClassifier
# start = time.time()
# model = GradientBoostingClassifier().fit(X_train,y_train)
# end_train = time.time()
# y_predictions = model.predict(X_test) # These are the predictions from the test data.
# end_predict = time.time()

accuracy = accuracy_score(y_test, y_predictions)
recall = recall_score(y_test, y_predictions, average='weighted')
precision = precision_score(y_test, y_predictions, average='weighted')
f1s = f1_score(y_test, y_predictions, average='weighted')
MCC = matthews_corrcoef(y_test, y_predictions)
# ROC_AUC = roc_auc_score(y_test, y_predictions, average='weighted')
ROC_AUC = roc_auc_score(y_test, model.predict_proba(X_test)[:,1], average='weighted')

print("Accuracy: "+ "{:.2%}".format(accuracy))
print("Recall: "+ "{:.2%}".format(recall))
print("Precision: "+ "{:.2%}".format(precision))
print("F1-Score: "+ "{:.2%}".format(f1s))
print("MCC: "+ "{:.2%}".format(MCC))
print("ROC AUC score: "+ "{:.2%}".format(ROC_AUC))
print("time to train: "+ "{:.2f}".format(end_train-start)+" s")
print("time to predict: "+"{:.2f}".format(end_predict-end_train)+" s")
print("total: "+"{:.2f}".format(end_predict-start)+" s")
model_performance.loc['Gradient Boosting Classifier'] = [accuracy, recall, precision, f1s,MCC,end_train-start,end_predict-end_train,end_predict-start]

plt.rcParams['figure.figsize']=5,5
sns.set_style("white")

# Instead of plot_confusion_matrix, use ConfusionMatrixDisplay.from_estimator: because it was giving me error
from sklearn.metrics import ConfusionMatrixDisplay  # Ensure this is imported
ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=plt.cm.Blues)

plt.show()

plt.rcParams['figure.figsize']=10,10
sns.set_style("white")
feat_importances = pd.Series(model.feature_importances_, index=feature_names)
feat_importances = feat_importances.groupby(level=0).mean()
feat_importances.nlargest(20).plot(kind='barh').invert_yaxis()
sns.despine()
plt.show()

"""**Types of Perceptrons**

### **1. Single-Layer, Single Perceptron**
A single perceptron in a single-layer network consists of only one neuron. It processes inputs and produces an output using a simple activation function like the step function. This type of perceptron is used for binary classification of linearly separable data, such as logic gates (AND, OR). However, it cannot solve non-linearly separable problems like XOR.

**Key Features:**
- Only one neuron
- Used for simple binary classification
- Cannot handle non-linearly separable data
- Typically uses a step function or sigmoid activation

**Use Case:** Simple binary classification tasks, such as AND/OR logic gates.

---

### **2. Single-Layer, Multi Perceptron**
A single-layer perceptron with multiple neurons operates in parallel, allowing classification of multiple categories. It extends the single perceptron by introducing multiple output neurons. This enables the model to handle multi-class problems where the output is not just binary but has multiple possible labels.

**Key Features:**
- Single layer with multiple neurons
- Used for multi-class classification
- Typically employs softmax activation for probability distribution
- Can process multiple features simultaneously

**Use Case:** Multi-class classification problems, such as digit recognition or document classification.

---

### **3. Multi-Layer, Multi Perceptron (MLP)**
A multi-layer perceptron (MLP) consists of multiple layers, including one or more hidden layers. This structure enables the model to learn complex, non-linear patterns and solve problems that single-layer perceptrons cannot. MLPs use backpropagation for training, allowing them to adjust weights based on errors and improve accuracy.

**Key Features:**
- Multiple layers (input, hidden, and output layers)
- Can learn non-linear relationships
- Uses backpropagation for training
- Common activation functions include ReLU, sigmoid, and tanh

**Use Case:** Used in deep learning applications such as image recognition, speech processing, and financial forecasting.

---

### **Comparison Table**

| Type | Layers | Neurons | Use Case |
|-------|--------|---------|----------------|
| **Single-Layer, Single Perceptron** | 1 | 1 | Binary classification (AND, OR) |
| **Single-Layer, Multi Perceptron** | 1 | Multiple | Multi-class classification |
| **Multi-Layer, Multi Perceptron (MLP)** | Multiple | Multiple | Complex problems (XOR, image recognition) |

Each perceptron type has its strengths and limitations. While a single-layer perceptron is fast and simple, multi-layer perceptrons are essential for solving real-world AI problems.

<a id='4_7'></a>
## <p style="padding: 8px;color:white; display:fill;background-color:#aaaaaa; border-radius:5px; font-size:100%"> <b>Neural Network MLP</b>
"""

import time
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (accuracy_score, recall_score, precision_score,
                             f1_score, matthews_corrcoef, roc_auc_score,
                             ConfusionMatrixDisplay)

# Assuming X_train, X_test, y_train, y_test are already defined

# Parameters
max_epochs = 1000  # Number of epochs

start = time.time()
model = MLPClassifier(
    hidden_layer_sizes=(1,),  # Single-layer, single perceptron
    activation='relu',  # relu activation function
    solver='sgd',  # Stochastic gradient descent solver
    max_iter=max_epochs,  # Maximum number of iterations (epochs)
    random_state=0,
    verbose=True  # Enables logging of training progress
)

# Training the model
model.fit(X_train, y_train)
end_train = time.time()

# Predictions
y_predictions = model.predict(X_test)
end_predict = time.time()

# Metrics Calculation
accuracy = accuracy_score(y_test, y_predictions)
recall = recall_score(y_test, y_predictions, average='weighted')
precision = precision_score(y_test, y_predictions, average='weighted')
f1s = f1_score(y_test, y_predictions, average='weighted')
MCC = matthews_corrcoef(y_test, y_predictions)

# Handle potential error for roc_auc_score if predict_proba isn't available.
try:
    ROC_AUC = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1], average='weighted')
except AttributeError:
    print("Warning: predict_proba not available for ROC AUC calculation. Using default value.")
    ROC_AUC = 0  # Or handle differently

# Print results
print("Epochs trained:", model.n_iter_)
print("Accuracy:", "{:.2%}".format(accuracy))
print("Recall:", "{:.2%}".format(recall))
print("Precision:", "{:.2%}".format(precision))
print("F1-Score:", "{:.2%}".format(f1s))
print("MCC:", "{:.2%}".format(MCC))
print("ROC AUC score:", "{:.2%}".format(ROC_AUC))
print("Time to train:", "{:.2f}".format(end_train - start), "s")
print("Time to predict:", "{:.2f}".format(end_predict - end_train), "s")
print("Total time:", "{:.2f}".format(end_predict - start), "s")

# Confusion Matrix
plt.rcParams['figure.figsize'] = (5, 5)
sns.set_style("white")
ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=plt.cm.Blues)
plt.show()

"""after 36 epochs, training loss didnt improve, so best number of epochs for this is

solution: adjusting tol, increasing max_iter, or changing the optimizer to improve convergence.
"""

import time
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, matthews_corrcoef, roc_auc_score
from sklearn.metrics import ConfusionMatrixDisplay

# Assuming X_train, X_test, y_train, y_test are already defined

start = time.time()
model = MLPClassifier(hidden_layer_sizes=(1,),  # Single-layer, single perceptron
                      activation='relu',  # ReLU activation function
                      solver='sgd',  # Stochastic gradient descent solver
                      max_iter=500,  # Increased max_iter to allow more epochs
                      tol=1e-6,  # Lower tolerance for better convergence
                      random_state=0,
                      verbose=True).fit(X_train, y_train)
end_train = time.time()

# Predictions
y_predictions = model.predict(X_test)
end_predict = time.time()

# Metrics Calculation
accuracy = accuracy_score(y_test, y_predictions)
recall = recall_score(y_test, y_predictions, average='weighted')
precision = precision_score(y_test, y_predictions, average='weighted')
f1s = f1_score(y_test, y_predictions, average='weighted')
MCC = matthews_corrcoef(y_test, y_predictions)

# Handle potential error for roc_auc_score if predict_proba isn't available.
try:
    ROC_AUC = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1], average='weighted')
except AttributeError:
    print("Warning: predict_proba not available for ROC AUC calculation. Using default value.")
    ROC_AUC = 0  # Default value

# Display Results
print(f"Epochs trained: {model.n_iter_}")
print("Accuracy:", "{:.2%}".format(accuracy))
print("Recall:", "{:.2%}".format(recall))
print("Precision:", "{:.2%}".format(precision))
print("F1-Score:", "{:.2%}".format(f1s))
print("MCC:", "{:.2%}".format(MCC))
print("ROC AUC score:", "{:.2%}".format(ROC_AUC))
print("Time to train:", "{:.2f}".format(end_train - start), "s")
print("Time to predict:", "{:.2f}".format(end_predict - end_train), "s")
print("Total time:", "{:.2f}".format(end_predict - start), "s")

# Confusion Matrix
plt.rcParams['figure.figsize'] = (5, 5)
sns.set_style("white")
ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=plt.cm.Blues)
plt.show()

"""The 500-epoch model significantly improved the **ROC AUC score (78.21% vs. 30.72%)** but took **8.63s vs. 0.63s** to train, with all other metrics remaining the same. The 35-epoch model trained much faster but failed to generalize well in distinguishing between classes.

**Single layer Multi Perceptron**

Some suggestions:

max_iter (Epochs)

1000 (Default)	Best for complex problems or convergence issues

500	Moderate training time, usually enough for good results

200	If you want faster training (but risk underfitting)

50-100	Quick testing to check model setup
"""

import time
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (
    accuracy_score, recall_score, precision_score, f1_score,
    matthews_corrcoef, roc_auc_score, ConfusionMatrixDisplay
)

# Assuming X_train, X_test, y_train, y_test are already defined

start = time.time()
model = MLPClassifier(
    hidden_layer_sizes=(10,),  # Single hidden layer with 10 perceptrons
    activation='relu',  # Better for multi-perceptron networks
    solver='adam',  # Adaptive optimizer for better convergence
    max_iter=1000,  # Number of epochs
    verbose=True,  # Logs training progress
    random_state=0
).fit(X_train, y_train)

end_train = time.time()

# Predictions
y_predictions = model.predict(X_test)
end_predict = time.time()

# Metrics Calculation
accuracy = accuracy_score(y_test, y_predictions)
recall = recall_score(y_test, y_predictions, average='weighted')
precision = precision_score(y_test, y_predictions, average='weighted')
f1s = f1_score(y_test, y_predictions, average='weighted')
MCC = matthews_corrcoef(y_test, y_predictions)

# Handle potential error for ROC AUC
try:
    ROC_AUC = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1], average='weighted')
except AttributeError:
    print("Warning: predict_proba not available for ROC AUC calculation. Using default value.")
    ROC_AUC = 0

# Display Metrics
print("Accuracy:", "{:.2%}".format(accuracy))
print("Recall:", "{:.2%}".format(recall))
print("Precision:", "{:.2%}".format(precision))
print("F1-Score:", "{:.2%}".format(f1s))
print("MCC:", "{:.2%}".format(MCC))
print("ROC AUC score:", "{:.2%}".format(ROC_AUC))
print("Time to train:", "{:.2f}".format(end_train - start), "s")
print("Time to predict:", "{:.2f}".format(end_predict - end_train), "s")
print("Total time:", "{:.2f}".format(end_predict - start), "s")
print("Epochs Run:", model.n_iter_)  # Number of epochs actually completed

# Confusion Matrix Plot
plt.rcParams['figure.figsize'] = 5, 5
sns.set_style("white")
ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=plt.cm.Blues)
plt.show()

"""**Multilayer Multi Perceptron**"""

import time
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, matthews_corrcoef, roc_auc_score
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming X_train, X_test, y_train, y_test are already defined

start = time.time()
model = MLPClassifier(
    hidden_layer_sizes=(64, 32, 16),  # Three hidden layers (Multi-Layer)
    activation='relu',  # Best for multi-layer perceptrons
    solver='adam',  # Adaptive learning rate optimization
    max_iter=500,  # Number of epochs
    verbose=True,  # Show training progress
    random_state=0
).fit(X_train, y_train)
end_train = time.time()

y_predictions = model.predict(X_test)
end_predict = time.time()

# Evaluation Metrics
accuracy = accuracy_score(y_test, y_predictions)
recall = recall_score(y_test, y_predictions, average='weighted')
precision = precision_score(y_test, y_predictions, average='weighted')
f1s = f1_score(y_test, y_predictions, average='weighted')
MCC = matthews_corrcoef(y_test, y_predictions)

# Handle potential error for roc_auc_score if predict_proba isn't available
try:
    ROC_AUC = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1], average='weighted')
except AttributeError:
    print("Warning: predict_proba not available for ROC AUC calculation. Using default value.")
    ROC_AUC = 0  # Default value

# Print results
print("Accuracy:", "{:.2%}".format(accuracy))
print("Recall:", "{:.2%}".format(recall))
print("Precision:", "{:.2%}".format(precision))
print("F1-Score:", "{:.2%}".format(f1s))
print("MCC:", "{:.2%}".format(MCC))
print("ROC AUC score:", "{:.2%}".format(ROC_AUC))
print("Time to train:", "{:.2f}".format(end_train - start), "s")
print("Time to predict:", "{:.2f}".format(end_predict - end_train), "s")
print("Total time:", "{:.2f}".format(end_predict - start), "s")

# Plot Confusion Matrix
plt.rcParams['figure.figsize'] = (5, 5)
sns.set_style("white")
ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=plt.cm.Blues)
plt.show()

import plotly.graph_objects as go

# Assuming 'model_performance' DataFrame is already populated

fig = go.Figure()

# Add traces for each model
fig.add_trace(go.Bar(x=model_performance.index, y=model_performance['Accuracy'], name='Accuracy'))
fig.add_trace(go.Bar(x=model_performance.index, y=model_performance['Recall'], name='Recall'))
fig.add_trace(go.Bar(x=model_performance.index, y=model_performance['Precision'], name='Precision'))
fig.add_trace(go.Bar(x=model_performance.index, y=model_performance['F1-Score'], name='F1-Score'))
fig.add_trace(go.Bar(x=model_performance.index, y=model_performance['MCC score'], name='MCC Score'))
fig.add_trace(go.Bar(x=model_performance.index, y=model_performance['time to train'], name='Training Time'))
fig.add_trace(go.Bar(x=model_performance.index, y=model_performance['time to predict'], name='Prediction Time'))
fig.add_trace(go.Bar(x=model_performance.index, y=model_performance['total time'], name='Total Time'))


fig.update_layout(title='Model Performance Comparison',
                  xaxis_title='Model',
                  yaxis_title='Score',
                  barmode='group')  # Grouped bar chart

fig.show()

"""### 🔹 **Why ReLU for my Dataset?**  

My dataset involves **machine failure prediction**, which is a complex, **non-linear classification task** with continuous numerical features like temperature, torque, and rotational speed. Here's why **ReLU is a good choice for this data**:

#### ✅ **1. Handles Non-Linearity Well**  
- My dataset contains **complex, multi-dimensional relationships** between features (e.g., temperature, torque, rotational speed).  
- **ReLU introduces non-linearity**, allowing the model to **capture intricate failure patterns**, unlike sigmoid or tanh, which struggle with deep layers.  

#### ✅ **2. Efficient Training for Large-Scale Data**  
- My dataset has **10,000 rows and 14 features**, making it relatively large.  
- **ReLU does not saturate for positive values**, which helps in **faster and stable training** compared to sigmoid/tanh, which can lead to **vanishing gradients** in deep networks.  
- This allows efficient weight updates, making ReLU ideal for **both shallow and deep perceptrons** in my case.  

#### ✅ **3. Robust to Sensor Readings & Noise**  
- **My dataset has sensor-based features** like air temperature, process temperature, and rotational speed, which can have **fluctuations**.  
- **ReLU is robust** to slight variations in input values, unlike sigmoid/tanh, which are sensitive to small changes and may cause instability in training.  

#### ✅ **4. Works Well for Multi-Perceptron Models**  
- My **Multi-Layer Perceptron (MLP)** models benefit from ReLU because:  
  - It prevents neurons from becoming inactive (dying neurons are rare compared to sigmoid).  
  - It allows multiple layers to learn useful hierarchical features about machine failure.  
  - It speeds up training without complex calculations like exponentials in sigmoid/tanh.  

### 🔥 **Could Sigmoid/Tanh Work Instead?**  
❌ **Sigmoid:** Not ideal because it squashes large values to 0 or 1, making training slower (vanishing gradient issue).  
❌ **Tanh:** Better than sigmoid but still suffers from saturation for large values.  

### 🎯 **Conclusion:** **ReLU** is the best activation function for my dataset because it efficiently learns failure patterns, handles sensor data fluctuations, and allows deep networks to train effectively. 🚀

Pickling
"""

import pickle

# Assuming all the variables you want to save are in the current environment
#  e.g., model, X_train, y_train, etc.
# Create a dictionary to hold these variables
variables_to_save = {
    'model': model,
    'X_train': X_train,
    'y_train': y_train,
    'X_test': X_test,
    'y_test': y_test,
    # Add any other variables here
}


# Specify the filename for the pickle file
filename = 'variables.pkl'

# Open the file in write binary mode ('wb') and save the variables using pickle
with open(filename, 'wb') as file:
  pickle.dump(variables_to_save, file)